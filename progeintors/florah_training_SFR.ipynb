{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import florah\n",
    "from florah import utils\n",
    "from florah.models import rnn_model\n",
    "from florah.models.rnn_model import rnn_generator\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "\n",
    "data_path = \"/Users/marchuertascompany/Documents/data/CEERS/TNG100/\"\n",
    "hdf5_file_path = data_path+\"projTNGEAGLESimbamstargt9_random_sizemassSFR.h5\"\n",
    "# Initialize 'x' and 't' as lists to store the cleaned data\n",
    "x = []\n",
    "t = []\n",
    "node_features = {'x': None, 't': None}\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "    # Loop through the groups (indexed by integers)\n",
    "    for group_name in hdf5_file:\n",
    "        group = hdf5_file[group_name]\n",
    "        \n",
    "        # Read the 'x' and 't' data from the group\n",
    "        x_data = group['x'][:]\n",
    "        t_data = 1/(1+group['z'][:])\n",
    "        z_data = group['z'][:]\n",
    "        \n",
    "        # Convert the 'x_data' to a list of floats while ignoring non-numeric and 'inf' values and skipping the first row\n",
    "        cleaned_x_mass = [float(value) for value,size,sfr in zip(x_data[1:,0],x_data[1:,1],x_data[1:,2]) if value != b'-' and value != b'-inf' and size>0 and sfr>=0]\n",
    "        cleaned_x_size = [float(value/(1+z)) for value,z,sfr in zip(x_data[1:,1],z_data,x_data[1:,2]) if value != b'-' and value != b'-inf' and value >0 and sfr>=0]\n",
    "        cleaned_x_SFR = [float(value)+1e-5 for value,size in zip(x_data[1:,2],x_data[1:,1]) if value != b'-' and value != b'-inf' and size >0 and value>=0]\n",
    "        \n",
    "        #print(np.array(cleaned_x_mass).shape)\n",
    "        x_copy = np.column_stack([cleaned_x_mass, np.log10(cleaned_x_size),np.log10(cleaned_x_SFR)])\n",
    "        #print(np.array(x_copy).shape)\n",
    "        #cleaned_x = [float(value) for value in x_data[1:] if value != b'-' and value != b'-inf']\n",
    "        # Convert the 't_data' to a list of floats while ignoring non-numeric and 'inf' values and skipping the first row\n",
    "        cleaned_t = [float(value) for value,size,sfr in zip(t_data[1:],x_data[1:,1],x_data[1:,2]) if value != b'-' and value != b'-inf' and size>0 and sfr>=0]\n",
    "        #print(np.array(cleaned_t).shape)\n",
    "        cleaned_t = np.expand_dims(cleaned_t,1)\n",
    "        \n",
    "       # Append the cleaned 'x' and 't' data to their respective lists\n",
    "        if len(cleaned_t)>0:\n",
    "            x.append(x_copy)\n",
    "            t.append(cleaned_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706318\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(x[568]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in range(len(x)):\n",
    "    sh = x[i].shape[0]\n",
    "    if sh <1:\n",
    "        print('caution',i) \n",
    "        k+=1\n",
    "print(k)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(t[2380].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Store 'x_copy' and 't' data as lists of NumPy arrays in the 'node_features' dictionary\n",
    "node_features = {'x': [np.array(arr, dtype=np.float32) for arr in x], 't': [np.array(arr, dtype=np.float32) for arr in t]}\n",
    "\n",
    "# Now, 'x' and 't' contain cleaned and converted data as NumPy arrays of objects\n",
    "\n",
    "\n",
    "x = node_features['x']   # stellar mass and half mass radius\n",
    "t = node_features['t']   # scale factor\n",
    "\n",
    "# Split the data into training (85%) and validation (15%) sets\n",
    "x_train, x_val, t_train, t_val = train_test_split(x, t, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Store 'x_copy' and 't' data as lists of NumPy arrays in the 'node_features' dictionary\n",
    "node_features_train = {'x': [np.array(arr, dtype=np.float32) for arr in x_train], 't': [np.array(arr, dtype=np.float32) for arr in t_train]}\n",
    "node_features_val = {'x': [np.array(arr, dtype=np.float32) for arr in x_val], 't': [np.array(arr, dtype=np.float32) for arr in t_val]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "channel 3\n",
      "(600370, 18, 3)\n",
      "here [0]\n",
      "torch.Size([600370, 18, 3])\n",
      "torch.Size([600370, 18, 3])\n",
      "torch.Size([600370, 19, 1])\n",
      "done\n",
      "channel 3\n",
      "(105948, 18, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here [0]\n",
      "torch.Size([105948, 18, 3])\n",
      "torch.Size([105948, 18, 3])\n",
      "torch.Size([105948, 19, 1])\n",
      "done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 85\u001b[0m\n\u001b[1;32m     67\u001b[0m trainer  \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     68\u001b[0m     default_root_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/scratch/mhuertas/CEERS/proj/TNGEagleSimba_mass_size_gt9\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     86\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m     87\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mdata_loader_val)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:956\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m    955\u001b[0m \u001b[39m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msetup(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    958\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py:75\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, trainer: pl\u001b[39m.\u001b[39mTrainer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_to_device()\n\u001b[1;32m     76\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msetup(trainer)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py:72\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_to_device\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mself.model must be set before self.model.to()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_device)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:54\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)[:\u001b[39m2\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_properties(device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:844\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mfor\u001b[39;00m key, buf \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    843\u001b[0m     \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffers[key] \u001b[39m=\u001b[39m fn(buf)\n\u001b[1;32m    846\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "#### TRAININIG\n",
    "\n",
    "# define hyperparameters\n",
    "# model architecture\n",
    "model_hparams = dict(\n",
    "    in_channels=3,   # number of input channels, in this case it is the halo mass and concentration\n",
    "    out_channels=3,   # number of output channels, in this case it is also the halo mass and concentration\n",
    "    num_layers=4,\n",
    "    hidden_features=128,\n",
    "    num_layers_flows=4,\n",
    "    hidden_features_flows=128,\n",
    "    num_blocks=2,\n",
    "    rnn_name=\"GRU\",\n",
    ")\n",
    "# optimizer\n",
    "optimizer_hparams = dict(\n",
    "    optimizer=dict(\n",
    "        optimizer=\"AdamW\",\n",
    "        lr=5e-4,\n",
    "        betas=(0.9, 0.98)\n",
    "    ),\n",
    "    scheduler=dict(\n",
    "        scheduler=\"ReduceLROnPlateau\",\n",
    "        patience=10\n",
    "    )\n",
    ")\n",
    "# time series preprocessing transformation\n",
    "transform_hparams = dict(\n",
    "    nx=3, \n",
    "    ny=3,\n",
    "    sub_dim=[0])\n",
    "\n",
    "# Now we can create the model. The model is a Pytorch Lightning module, which \n",
    "# will store the hyperparameters and the optimizer.\n",
    "model = rnn_generator.DataModule(\n",
    "    model_hparams, transform_hparams, optimizer_hparams\n",
    ")\n",
    "\n",
    "\n",
    "# preprocess the data and create DataLoader\n",
    "# new node features: x, y, t, seq_len, mask\n",
    "# x: stellar mass and size at the current time step (normalized)\n",
    "# y: accreted mass and size at next time step (normalized)\n",
    "# t: normalized time\n",
    "# seq_len: length of each time series\n",
    "# mask: mask for padding\n",
    "preprocessed_node_features = model.transform(node_features_train, fit=True)\n",
    "preprocessed_node_features_val = model.transform(node_features_val, fit=True)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(*preprocessed_node_features)\n",
    "dataset_val = torch.utils.data.TensorDataset(*preprocessed_node_features_val)\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1024, shuffle=True, num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else 'mps'\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1024, shuffle=False, num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else 'mps'\n",
    ")\n",
    "\n",
    "# Create a Pytorch lightning trainer. This will handle the training loop and\n",
    "# checkpointing.\n",
    "trainer  = pl.Trainer(\n",
    "    default_root_dir=\"/scratch/mhuertas/CEERS/proj/TNGEagleSimba_mass_size_gt9\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=500,\n",
    "    logger=pl.loggers.CSVLogger(\"delta_run\", name=\"delta_run\"),\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath='/scratch/mhuertas/CEERS/proj/TNGEagleSimba_mass_size_gt9/SFR_val/',filename=\"{epoch}-{val_loss:.4f}\", save_weights_only=False,\n",
    "            mode=\"min\", monitor=\"val_loss\",save_top_k=5,save_last=True,every_n_epochs = 1),\n",
    "        pl.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "    ],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.fit(\n",
    "    model=model, train_dataloaders=data_loader,\n",
    "    val_dataloaders=data_loader_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
