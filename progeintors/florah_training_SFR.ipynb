{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import florah\n",
    "from florah import utils\n",
    "from florah.models import rnn_model\n",
    "from florah.models.rnn_model import rnn_generator\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "\n",
    "data_path = \"/Users/marchuertascompany/Documents/data/CEERS/TNG100/\"\n",
    "hdf5_file_path = data_path+\"projTNGEAGLEmstargt9_random_sizemassSFR.h5\"\n",
    "# Initialize 'x' and 't' as lists to store the cleaned data\n",
    "x = []\n",
    "t = []\n",
    "node_features = {'x': None, 't': None}\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "    # Loop through the groups (indexed by integers)\n",
    "    for group_name in hdf5_file:\n",
    "        group = hdf5_file[group_name]\n",
    "        \n",
    "        # Read the 'x' and 't' data from the group\n",
    "        x_data = group['x'][:]\n",
    "        t_data = 1/(1+group['z'][:])\n",
    "        z_data = group['z'][:]\n",
    "        \n",
    "        # Convert the 'x_data' to a list of floats while ignoring non-numeric and 'inf' values and skipping the first row\n",
    "        cleaned_x_mass = [float(value) for value,size,sfr in zip(x_data[1:,0],x_data[1:,1],x_data[1:,2]) if value != b'-' and value != b'-inf' and size>0 and sfr>=0]\n",
    "        cleaned_x_size = [float(value/(1+z)) for value,z,sfr in zip(x_data[1:,1],z_data,x_data[1:,2]) if value != b'-' and value != b'-inf' and value >0 and sfr>=0]\n",
    "        cleaned_x_SFR = [float(value)+1e-5 for value,size in zip(x_data[1:,2],x_data[1:,1]) if value != b'-' and value != b'-inf' and size >0 and value>=0]\n",
    "        \n",
    "        #print(np.array(cleaned_x_mass).shape)\n",
    "        x_copy = np.column_stack([cleaned_x_mass, np.log10(cleaned_x_size),np.log10(cleaned_x_SFR)])\n",
    "        #print(np.array(x_copy).shape)\n",
    "        #cleaned_x = [float(value) for value in x_data[1:] if value != b'-' and value != b'-inf']\n",
    "        # Convert the 't_data' to a list of floats while ignoring non-numeric and 'inf' values and skipping the first row\n",
    "        cleaned_t = [float(value) for value,size,sfr in zip(t_data[1:],x_data[1:,1],x_data[1:,2]) if value != b'-' and value != b'-inf' and size>0 and sfr>=0]\n",
    "        #print(np.array(cleaned_t).shape)\n",
    "        cleaned_t = np.expand_dims(cleaned_t,1)\n",
    "        \n",
    "       # Append the cleaned 'x' and 't' data to their respective lists\n",
    "        x.append(x_copy)\n",
    "        t.append(cleaned_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324231\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in range(len(x)):\n",
    "    sh = x[i].shape[0]\n",
    "    if sh <1:\n",
    "        print('caution',i) \n",
    "        k+=1\n",
    "print(k)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.49138975  0.44108861 -5.        ]\n",
      " [ 6.49397516  0.46639055 -5.        ]\n",
      " [ 6.49866438  0.05347586 -5.        ]\n",
      " [ 6.72391415  0.29764018 -5.        ]\n",
      " [ 6.72975969  0.21173919 -5.        ]\n",
      " [ 6.73786545  0.21448414 -5.        ]\n",
      " [ 6.74420357 -0.15410475 -5.        ]\n",
      " [ 6.89937639  0.1153261  -5.        ]\n",
      " [ 6.77240705 -0.25253089 -5.        ]\n",
      " [ 6.87818623 -0.4523753  -5.        ]\n",
      " [ 6.84237671 -0.21280798 -5.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x[2380])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Store 'x_copy' and 't' data as lists of NumPy arrays in the 'node_features' dictionary\n",
    "node_features = {'x': [np.array(arr, dtype=np.float32) for arr in x], 't': [np.array(arr, dtype=np.float32) for arr in t]}\n",
    "\n",
    "# Now, 'x' and 't' contain cleaned and converted data as NumPy arrays of objects\n",
    "\n",
    "\n",
    "x = node_features['x']   # stellar mass and half mass radius\n",
    "t = node_features['t']   # scale factor\n",
    "\n",
    "# Split the data into training (85%) and validation (15%) sets\n",
    "x_train, x_val, t_train, t_val = train_test_split(x, t, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Store 'x_copy' and 't' data as lists of NumPy arrays in the 'node_features' dictionary\n",
    "node_features_train = {'x': [np.array(arr, dtype=np.float32) for arr in x_train], 't': [np.array(arr, dtype=np.float32) for arr in t_train]}\n",
    "node_features_val = {'x': [np.array(arr, dtype=np.float32) for arr in x_val], 't': [np.array(arr, dtype=np.float32) for arr in t_val]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "channel 3\n",
      "(275596, 18, 3)\n",
      "here [0]\n",
      "torch.Size([275596, 18, 3])\n",
      "torch.Size([275596, 18, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marchuertascompany/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | model     | RecurrentMAF | 554 K \n",
      "1 | transform | Preprocess   | 0     \n",
      "-------------------------------------------\n",
      "554 K     Trainable params\n",
      "0         Non-trainable params\n",
      "554 K     Total params\n",
      "2.218     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([275596, 19, 1])\n",
      "done\n",
      "channel 3\n",
      "(48635, 18, 3)\n",
      "here [0]\n",
      "torch.Size([48635, 18, 3])\n",
      "torch.Size([48635, 18, 3])\n",
      "torch.Size([48635, 19, 1])\n",
      "done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c40b8f22256431490cc50710d97d928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13d78b9f93e42a89e095c1a79fdc444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0e8c83a2a4945810c0acef71b7228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/scratch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 85\u001b[0m\n\u001b[1;32m     67\u001b[0m trainer  \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     68\u001b[0m     default_root_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/scratch/mhuertas/CEERS/proj/TNGEagle_mass_size_gt9\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     86\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m     87\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mdata_loader_val)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:203\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:370\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    369\u001b[0m call\u001b[39m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m call\u001b[39m.\u001b[39;49m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, monitoring_callbacks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    372\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    375\u001b[0m     \u001b[39m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:195\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    194\u001b[0m         \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 195\u001b[0m             fn(trainer, trainer\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:303\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    301\u001b[0m monitor_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_candidates(trainer)\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (trainer\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_last_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360\u001b[0m, in \u001b[0;36mModelCheckpoint._save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[39mraise\u001b[39;00m MisconfigurationException(m)\n\u001b[1;32m    359\u001b[0m         warning_cache\u001b[39m.\u001b[39mwarn(m)\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_monitor_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:663\u001b[0m, in \u001b[0;36mModelCheckpoint._save_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_monitor_top_k(trainer, current):\n\u001b[1;32m    662\u001b[0m     \u001b[39massert\u001b[39;00m current \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_best_and_save(current, trainer, monitor_candidates)\n\u001b[1;32m    664\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    665\u001b[0m     epoch \u001b[39m=\u001b[39m monitor_candidates[\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:714\u001b[0m, in \u001b[0;36mModelCheckpoint._update_best_and_save\u001b[0;34m(self, current, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    709\u001b[0m     step \u001b[39m=\u001b[39m monitor_candidates[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    710\u001b[0m     rank_zero_info(\n\u001b[1;32m    711\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, global step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor\u001b[39m!r}\u001b[39;00m\u001b[39m reached \u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (best \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_score\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m), saving model to \u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m!r}\u001b[39;00m\u001b[39m as top \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m     )\n\u001b[0;32m--> 714\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(trainer, filepath)\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m del_filepath \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filepath \u001b[39m!=\u001b[39m del_filepath:\n\u001b[1;32m    717\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_checkpoint(trainer, del_filepath)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:365\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, filepath: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     trainer\u001b[39m.\u001b[39;49msave_checkpoint(filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_weights_only)\n\u001b[1;32m    367\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_global_step_saved \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m    369\u001b[0m     \u001b[39m# notify loggers\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1316\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1314\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mfit,validate,test,predict}`?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1315\u001b[0m     )\n\u001b[0;32m-> 1316\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49msave_checkpoint(filepath, weights_only\u001b[39m=\u001b[39;49mweights_only, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:507\u001b[0m, in \u001b[0;36m_CheckpointConnector.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \n\u001b[1;32m    500\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    506\u001b[0m _checkpoint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m--> 507\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msave_checkpoint(_checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:466\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[1;32m    459\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \n\u001b[1;32m    464\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_io\u001b[39m.\u001b[39;49msave_checkpoint(checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/lightning_fabric/plugins/io/torch_io.py:56\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Trainer.save_checkpoint(..., storage_options=...)` with `storage_options` arg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m is not supported for `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`. Please implement your custom `CheckpointIO`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m to define how you\u001b[39m\u001b[39m'\u001b[39m\u001b[39md like to use `storage_options`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path)\n\u001b[0;32m---> 56\u001b[0m fs\u001b[39m.\u001b[39;49mmakedirs(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(path), exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     _atomic_save(checkpoint, path)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/site-packages/fsspec/implementations/local.py:54\u001b[0m, in \u001b[0;36mLocalFileSystem.makedirs\u001b[0;34m(self, path, exist_ok)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmakedirs\u001b[39m(\u001b[39mself\u001b[39m, path, exist_ok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m---> 54\u001b[0m     os\u001b[39m.\u001b[39;49mmakedirs(path, exist_ok\u001b[39m=\u001b[39;49mexist_ok)\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m head \u001b[39mand\u001b[39;00m tail \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         makedirs(head, exist_ok\u001b[39m=\u001b[39;49mexist_ok)\n\u001b[1;32m    214\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[39m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m head \u001b[39mand\u001b[39;00m tail \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         makedirs(head, exist_ok\u001b[39m=\u001b[39;49mexist_ok)\n\u001b[1;32m    214\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[39m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 213 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m head \u001b[39mand\u001b[39;00m tail \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         makedirs(head, exist_ok\u001b[39m=\u001b[39;49mexist_ok)\n\u001b[1;32m    214\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[39m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/soft/miniforge3/envs/tfenv23/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/scratch'"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "#### TRAININIG\n",
    "\n",
    "# define hyperparameters\n",
    "# model architecture\n",
    "model_hparams = dict(\n",
    "    in_channels=3,   # number of input channels, in this case it is the halo mass and concentration\n",
    "    out_channels=3,   # number of output channels, in this case it is also the halo mass and concentration\n",
    "    num_layers=4,\n",
    "    hidden_features=128,\n",
    "    num_layers_flows=4,\n",
    "    hidden_features_flows=128,\n",
    "    num_blocks=2,\n",
    "    rnn_name=\"GRU\",\n",
    ")\n",
    "# optimizer\n",
    "optimizer_hparams = dict(\n",
    "    optimizer=dict(\n",
    "        optimizer=\"AdamW\",\n",
    "        lr=5e-4,\n",
    "        betas=(0.9, 0.98)\n",
    "    ),\n",
    "    scheduler=dict(\n",
    "        scheduler=\"ReduceLROnPlateau\",\n",
    "        patience=10\n",
    "    )\n",
    ")\n",
    "# time series preprocessing transformation\n",
    "transform_hparams = dict(\n",
    "    nx=3, \n",
    "    ny=3,\n",
    "    sub_dim=[0])\n",
    "\n",
    "# Now we can create the model. The model is a Pytorch Lightning module, which \n",
    "# will store the hyperparameters and the optimizer.\n",
    "model = rnn_generator.DataModule(\n",
    "    model_hparams, transform_hparams, optimizer_hparams\n",
    ")\n",
    "\n",
    "\n",
    "# preprocess the data and create DataLoader\n",
    "# new node features: x, y, t, seq_len, mask\n",
    "# x: stellar mass and size at the current time step (normalized)\n",
    "# y: accreted mass and size at next time step (normalized)\n",
    "# t: normalized time\n",
    "# seq_len: length of each time series\n",
    "# mask: mask for padding\n",
    "preprocessed_node_features = model.transform(node_features_train, fit=True)\n",
    "preprocessed_node_features_val = model.transform(node_features_val, fit=True)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(*preprocessed_node_features)\n",
    "dataset_val = torch.utils.data.TensorDataset(*preprocessed_node_features_val)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1024, shuffle=True, num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1024, shuffle=False, num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Create a Pytorch lightning trainer. This will handle the training loop and\n",
    "# checkpointing.\n",
    "trainer  = pl.Trainer(\n",
    "    default_root_dir=\"/scratch/mhuertas/CEERS/proj/TNGEagle_mass_size_gt9\",\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=500,\n",
    "    logger=pl.loggers.CSVLogger(\"delta_run\", name=\"delta_run\"),\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath='/scratch/mhuertas/CEERS/proj/TNGEagle_mass_size_gt9/SFR_val/',filename=\"{epoch}-{val_loss:.4f}\", save_weights_only=False,\n",
    "            mode=\"min\", monitor=\"val_loss\",save_top_k=5,save_last=True,every_n_epochs = 1),\n",
    "        pl.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "    ],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.fit(\n",
    "    model=model, train_dataloaders=data_loader,\n",
    "    val_dataloaders=data_loader_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
